{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HTTP_PROXY'] = 'http://web-proxy.tencent.com:8080'\n",
    "os.environ['HTTPS_PROXY'] = 'https://web-proxy.tencent.com:8080'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models, callbacks, Input, preprocessing, applications\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.3 (default, Mar 27 2019, 17:13:21) [MSC v.1915 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(_, train_labels), (_, test_labels) = keras.datasets.reuters.load_data(\n",
    "num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(64, activation='relu', input_shape=(28*28,)))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "#model.add(keras.layers.Dropout(0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/10\n",
      "54000/54000 [==============================] - 1s 16us/sample - loss: 0.6051 - acc: 0.7858 - val_loss: 0.4852 - val_acc: 0.8272\n",
      "Epoch 2/10\n",
      "54000/54000 [==============================] - 1s 13us/sample - loss: 0.4232 - acc: 0.8469 - val_loss: 0.3990 - val_acc: 0.8545\n",
      "Epoch 3/10\n",
      "54000/54000 [==============================] - 1s 12us/sample - loss: 0.3754 - acc: 0.8627 - val_loss: 0.3734 - val_acc: 0.8660\n",
      "Epoch 4/10\n",
      "54000/54000 [==============================] - 1s 12us/sample - loss: 0.3485 - acc: 0.8731 - val_loss: 0.3663 - val_acc: 0.8632\n",
      "Epoch 5/10\n",
      "54000/54000 [==============================] - 1s 12us/sample - loss: 0.3289 - acc: 0.8793 - val_loss: 0.3627 - val_acc: 0.8703\n",
      "Epoch 6/10\n",
      "54000/54000 [==============================] - 1s 12us/sample - loss: 0.3107 - acc: 0.8864 - val_loss: 0.3377 - val_acc: 0.8757\n",
      "Epoch 7/10\n",
      "54000/54000 [==============================] - 1s 12us/sample - loss: 0.2989 - acc: 0.8892 - val_loss: 0.3584 - val_acc: 0.8742\n",
      "Epoch 8/10\n",
      "54000/54000 [==============================] - 1s 13us/sample - loss: 0.2898 - acc: 0.8942 - val_loss: 0.3372 - val_acc: 0.8785\n",
      "Epoch 9/10\n",
      "54000/54000 [==============================] - 1s 13us/sample - loss: 0.2803 - acc: 0.8967 - val_loss: 0.3518 - val_acc: 0.8770\n",
      "Epoch 10/10\n",
      "54000/54000 [==============================] - 1s 12us/sample - loss: 0.2712 - acc: 0.9006 - val_loss: 0.3299 - val_acc: 0.8808\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train.reshape(-1,28*28), y_train, validation_split=0.1, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test.reshape(-1,28,28,1))\n",
    "pred = np.argmax(predictions,axis=1)\n",
    "sum([i !=0 for i in y_test - pred]) / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[136], y_test[136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    './data/dogs-vs-cats/train_dir/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    './data/dogs-vs-cats/validation_dir/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 38s 1us/step\n"
     ]
    }
   ],
   "source": [
    "conv_base = keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "model = keras.models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(lr=2e-5),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 127s 1s/step - loss: 0.5085 - acc: 0.7800 - val_loss: 0.3949 - val_acc: 0.8510\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 127s 1s/step - loss: 0.3457 - acc: 0.8755 - val_loss: 0.3194 - val_acc: 0.8860\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 129s 1s/step - loss: 0.2853 - acc: 0.8955 - val_loss: 0.2901 - val_acc: 0.8940\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 131s 1s/step - loss: 0.2495 - acc: 0.9105 - val_loss: 0.2671 - val_acc: 0.8970\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 134s 1s/step - loss: 0.2213 - acc: 0.9270 - val_loss: 0.2562 - val_acc: 0.9030\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 127s 1s/step - loss: 0.2021 - acc: 0.9350 - val_loss: 0.2515 - val_acc: 0.8950\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 131s 1s/step - loss: 0.1846 - acc: 0.9395 - val_loss: 0.2649 - val_acc: 0.8890\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 125s 1s/step - loss: 0.1691 - acc: 0.9475 - val_loss: 0.2373 - val_acc: 0.9030\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.1568 - acc: 0.9500 - val_loss: 0.2340 - val_acc: 0.9020\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.1442 - acc: 0.9540 - val_loss: 0.2340 - val_acc: 0.9070\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 126s 1s/step - loss: 0.1353 - acc: 0.9575 - val_loss: 0.2486 - val_acc: 0.8950\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 125s 1s/step - loss: 0.1267 - acc: 0.9600 - val_loss: 0.2295 - val_acc: 0.9010\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.1179 - acc: 0.9625 - val_loss: 0.2340 - val_acc: 0.8950\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.1091 - acc: 0.9675 - val_loss: 0.2295 - val_acc: 0.9070\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.1022 - acc: 0.9695 - val_loss: 0.2309 - val_acc: 0.9020\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 132s 1s/step - loss: 0.0976 - acc: 0.9715 - val_loss: 0.2301 - val_acc: 0.9040\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.0900 - acc: 0.9725 - val_loss: 0.2362 - val_acc: 0.8950\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.0852 - acc: 0.9755 - val_loss: 0.2422 - val_acc: 0.8970\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0792 - acc: 0.9805 - val_loss: 0.2317 - val_acc: 0.9030\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 125s 1s/step - loss: 0.0739 - acc: 0.9830 - val_loss: 0.2366 - val_acc: 0.9010\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.0694 - acc: 0.9880 - val_loss: 0.2369 - val_acc: 0.9060\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0640 - acc: 0.9905 - val_loss: 0.2367 - val_acc: 0.9050\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0591 - acc: 0.9910 - val_loss: 0.2427 - val_acc: 0.9040\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.0555 - acc: 0.9930 - val_loss: 0.2468 - val_acc: 0.9010\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 124s 1s/step - loss: 0.0514 - acc: 0.9940 - val_loss: 0.2678 - val_acc: 0.8940\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0487 - acc: 0.9945 - val_loss: 0.2433 - val_acc: 0.9060\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0443 - acc: 0.9950 - val_loss: 0.2592 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 125s 1s/step - loss: 0.0425 - acc: 0.9960 - val_loss: 0.2517 - val_acc: 0.9060\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0388 - acc: 0.9965 - val_loss: 0.2521 - val_acc: 0.9100\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 123s 1s/step - loss: 0.0362 - acc: 0.9970 - val_loss: 0.2588 - val_acc: 0.9000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0417 17:48:12.856209 10936 deprecation.py:506] From D:\\Anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_46 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_18 (Embedding)     (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 496, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 165, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 161, 32)           5152      \n",
      "_________________________________________________________________\n",
      "unified_gru (UnifiedGRU)     (None, 32)                6336      \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 336,673\n",
      "Trainable params: 336,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(max_len,))\n",
    "x = layers.Embedding(max_features, 32)(input_tensor)\n",
    "x = layers.Conv1D(32, 5, activation='relu')(x)\n",
    "x = layers.MaxPool1D(3)(x)\n",
    "x = layers.Conv1D(32, 5, activation='relu')(x)\n",
    "x = layers.GRU(32, dropout=0.1, recurrent_dropout=0.5)(x)\n",
    "output_tensor = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        [(None, 500)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 496, 16)           2576      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 322,593\n",
      "Trainable params: 322,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(max_len,))\n",
    "x = layers.Embedding(max_features, 32)(input_tensor)\n",
    "x = layers.Conv1D(16, 5, activation='relu')(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "output_tensor = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_64 (InputLayer)           [(None, 31, 31, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 31, 31, 128)  512         input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 31, 31, 128)  512         input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 16, 16, 3)    0           input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 31, 31, 128)  147584      conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 16, 16, 128)  512         input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 16, 16, 128)  147584      conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 16, 16, 128)  3584        average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 16, 16, 128)  147584      conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 512)  0           conv2d_101[0][0]                 \n",
      "                                                                 conv2d_103[0][0]                 \n",
      "                                                                 conv2d_104[0][0]                 \n",
      "                                                                 conv2d_107[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 447,872\n",
      "Trainable params: 447,872\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(31,31,3,))\n",
    "branch_1 = layers.Conv2D(128, 1, activation='relu', strides=2)(input_tensor)\n",
    "branch_2 = layers.Conv2D(128, 1, activation='relu')(input_tensor)\n",
    "branch_2 = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_2)\n",
    "branch_3 = layers.AvgPool2D(3, strides=2, padding='same')(input_tensor)\n",
    "branch_3 = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_3)\n",
    "branch_4 = layers.Conv2D(128, 1, activation='relu')(input_tensor)\n",
    "branch_4 = layers.Conv2D(128, 3, activation='relu', padding='same')(branch_4)\n",
    "branch_4 = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same')(branch_4)\n",
    "output_tensor = layers.concatenate([branch_1, branch_2, branch_3, branch_4], axis=-1)\n",
    "model = models.Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_69 (InputLayer)           [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 256, 256, 128 3584        input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 256, 256, 128 147584      conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 256, 256, 128 147584      conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 256, 256, 128 0           conv2d_117[0][0]                 \n",
      "                                                                 conv2d_115[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 298,752\n",
      "Trainable params: 298,752\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(256,256,3,))\n",
    "x = layers.Conv2D(128, 3, activation='relu', padding='same')(input_tensor)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "y = layers.add([y, x])\n",
    "\n",
    "model = models.Model(input_tensor, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/10\n",
      "22500/22500 [==============================] - 32s 1ms/sample - loss: 0.1856 - acc: 0.7059 - val_loss: 0.1364 - val_acc: 0.8100\n",
      "Epoch 2/10\n",
      "22500/22500 [==============================] - 31s 1ms/sample - loss: 0.1145 - acc: 0.8427 - val_loss: 0.1209 - val_acc: 0.8340\n",
      "Epoch 3/10\n",
      "22500/22500 [==============================] - 31s 1ms/sample - loss: 0.0925 - acc: 0.8750 - val_loss: 0.1046 - val_acc: 0.8544\n",
      "Epoch 4/10\n",
      "22500/22500 [==============================] - 31s 1ms/sample - loss: 0.0761 - acc: 0.8997 - val_loss: 0.1074 - val_acc: 0.8504\n",
      "Epoch 5/10\n",
      "22500/22500 [==============================] - 31s 1ms/sample - loss: 0.0647 - acc: 0.9160 - val_loss: 0.0980 - val_acc: 0.8612\n",
      "Epoch 6/10\n",
      "22500/22500 [==============================] - 32s 1ms/sample - loss: 0.0540 - acc: 0.9305 - val_loss: 0.1193 - val_acc: 0.8448\n",
      "Epoch 7/10\n",
      "22500/22500 [==============================] - 32s 1ms/sample - loss: 0.0476 - acc: 0.9381 - val_loss: 0.1199 - val_acc: 0.8440\n",
      "Epoch 8/10\n",
      "22500/22500 [==============================] - 32s 1ms/sample - loss: 0.0384 - acc: 0.9518 - val_loss: 0.1019 - val_acc: 0.8672\n",
      "Epoch 9/10\n",
      "22500/22500 [==============================] - 32s 1ms/sample - loss: 0.0313 - acc: 0.9611 - val_loss: 0.1062 - val_acc: 0.8620\n",
      "Epoch 10/10\n",
      "22500/22500 [==============================] - 32s 1ms/sample - loss: 0.0256 - acc: 0.9680 - val_loss: 0.0954 - val_acc: 0.8808loss: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f3a5717d30>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_split=0.1, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(28*28,))\n",
    "x = layers.Dense(64, activation='relu')(input_tensor)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_70 (InputLayer)        [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 55,050\n",
      "Trainable params: 55,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model(input_tensor, output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_1 = Input(shape=(28*28,))\n",
    "x1 = layers.Dense(32, activation='relu')(input_tensor_1)\n",
    "\n",
    "input_tensor_2 = Input(shape=(28*28,))\n",
    "x2 = layers.Dense(32, activation='relu')(input_tensor_2)\n",
    "\n",
    "x = layers.concatenate([x1, x2], axis=-1)\n",
    "\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 32)           25120       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           25120       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64)           0           dense_21[0][0]                   \n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 10)           650         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 50,890\n",
      "Trainable params: 50,890\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Model([input_tensor_1, input_tensor_2], output_tensor)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50240"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*64+64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "  128/54000 [..............................] - ETA: 9s - loss: 0.0463 - acc: 0.9844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0417 20:49:42.220350 10936 callbacks.py:236] Method (on_train_batch_end) is slow compared to the batch update (0.200033). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.0385 - acc: 0.9917 - val_loss: 0.4522 - val_acc: 0.9723\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.0402 - acc: 0.9920 - val_loss: 0.4373 - val_acc: 0.9763\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 1s 20us/sample - loss: 0.0408 - acc: 0.9923 - val_loss: 0.4463 - val_acc: 0.9730\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 1s 21us/sample - loss: 0.0381 - acc: 0.9923 - val_loss: 0.5041 - val_acc: 0.9730\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 1s 24us/sample - loss: 0.0347 - acc: 0.9930 - val_loss: 0.4682 - val_acc: 0.9745\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 1s 19us/sample - loss: 0.0334 - acc: 0.9935 - val_loss: 0.4953 - val_acc: 0.9748\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 1s 19us/sample - loss: 0.0347 - acc: 0.9932 - val_loss: 0.4992 - val_acc: 0.9725\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train.reshape(-1,28*28), x_train.reshape(-1,28*28)],\n",
    "                    y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [callbacks.EarlyStopping(monitor='acc', patience=1),\n",
    "                 callbacks.ModelCheckpoint(filepath='./models/mnist_model.h5', monitor='val_acc', save_best_only=True),\n",
    "                 callbacks.TensorBoard(log_dir='log_dir', histogram_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_2 (Separabl (None, 62, 62, 32)        155       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_3 (Separabl (None, 29, 29, 64)        2400      \n",
      "=================================================================\n",
      "Total params: 2,555\n",
      "Trainable params: 2,555\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(64,64,3,))\n",
    "x = layers.SeparableConv2D(32, 3, activation='relu')(input_tensor)\n",
    "x = layers.MaxPool2D(2)(x)\n",
    "x = layers.SeparableConv2D(64, 3, activation='relu')(x)\n",
    "model = models.Model(input_tensor, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_path = 'img/golden_gate.jpg'\n",
    "style_reference_image_path = 'img/the-starry-night.jpg'\n",
    "\n",
    "width, heigth = preprocessing.image.load_img(target_image_path).size\n",
    "img_heigth = 400\n",
    "img_width = int(width * img_heigth / heigth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(979, 734, 533, 400)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "width, heigth, img_width, img_heigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocessing.image.load_img(target_image_path, target_size=(img_heigth, img_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocessing.image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = preprocessing.image.load_img(image_path, target_size=(img_heigth, img_width))\n",
    "    img = preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = applications.vgg19.preprocess_input(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deprocess_image(x):\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0418 11:18:14.818264 71600 deprecation.py:323] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:63: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = tf.constant(preprocess_image(target_image_path))\n",
    "style_reference_image = tf.constant(preprocess_image(style_reference_image_path))\n",
    "\n",
    "combination_image = tf.placeholder(np.float32, shape=(1,img_heigth, img_width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0418 11:37:10.354603 71600 deprecation.py:506] From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1257: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 187s 2us/step\n"
     ]
    }
   ],
   "source": [
    "input_tensor = tf.concat([target_image, style_reference_image, combination_image], axis=0)\n",
    "model = applications.vgg19.VGG19(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return tf.sum(tf.square(combination-base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (28, 28, 1)\n",
    "batch_size = 16\n",
    "latent_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_befor_flattening = (14,14,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = tf.random_normal(shape=(tf.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + tf.exp(z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_26 (InputLayer)           [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 28, 28, 32)   320         input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 14, 14, 64)   18496       conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 14, 14, 64)   36928       conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 12544)        0           conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 32)           401440      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 32)           1056        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 32)           1056        dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 32)           0           dense_12[0][0]                   \n",
      "                                                                 dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 496,224\n",
      "Trainable params: 496,224\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape=img_shape)\n",
    "x = layers.Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu', strides=2)(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "model = models.Model(input_img, z)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 32\n",
    "heigth = 32\n",
    "width = 32\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = Input(shape=(latent_dim,))\n",
    "x = layers.Dense(128*16*16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "generator_output = layers.Conv2D(channels, 7 , activation='tanh', padding='same')(x)\n",
    "generator = models.Model(generator_input, generator_output)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       6272      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 28800)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 28801     \n",
      "=================================================================\n",
      "Total params: 35,073\n",
      "Trainable params: 35,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input = Input(shape=(heigth,width,channels))\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "discriminator_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "discriminator = models.Model(discriminator_input, discriminator_output)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8),\n",
    "    loss='binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = models.Model(gan_input, gan_output)\n",
    "\n",
    "gan.compile(\n",
    "    optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8),\n",
    "    loss = 'binary_crossentropy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = unpickle('./data/cifar-10-batches-py/data_batch_1')\n",
    "x_train = np.array(raw_data[b'data'])\n",
    "y_train = np.array(raw_data[b'labels'])\n",
    "for i in range(2,6):\n",
    "    raw_data = unpickle('./data/cifar-10-batches-py/data_batch_'+str(i))\n",
    "    x_train = np.concatenate([x_train, np.array(raw_data[b'data'])])\n",
    "    y_train = np.concatenate([y_train, np.array(raw_data[b'labels'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[y_train==1]\n",
    "x_train = x_train.reshape(x_train.shape[0], channels, heigth, width).transpose([0,2,3,1]).astype('float32') / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 32, 32, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJG0lEQVR4nD2WS2/cZxWHz3lv/8vM2DNjj29jJ3HSENTSFsqtFAqUgAQIJECw5COwYAuIDwALkECABBJ7EEKUmwSskGi5tVHTNEmbtonj2B5fxjOe+d/f95zDIsAXePT7PasHf/G7F5g5iSIXx6yjIMqA1gSWAUTEKI8iAIoExAYvpAgQAEBERAQQmIUABUBEmJmIAEAAgogIG0YwkW2Y8rO5baG2CQgyYEChyldnpYsjAs7KTGHUbi0KMBMh/peIAszCgCLAzCJCRIjIICzCzGaWZ977k+Pxg70jHbfanV6kIkFogmcfinmW2AgUz5t50+DF7cuPXDqfxDEzMzMgCCCjgPzvEAAAIKICZGAAMC/8/cUszxTYspaKxtaNNStCqCQQSsvFCZo40qSaPPf/vn7t6GT/4vb28vJykqbCQkQsjKzgf3QAEGZBfGjMTLNSBBHEOJui0co4cBVQADUv8jLPI9RtibQBGyVVVr21u7dzMOouLG5tbg6Wl7q9nlFaCz+cTwIMKCIizCLMYsqGrTUAKOQFPGpCgcZX3kAnbc9nxawpa2bnXMeJ1i4PtWZVn5xNp1mrnayvb1zavth2UeSc994zCGgWfiiMBExZV7VXiBjHsQAIAqMwSp5ncYKR1eSxqsuALChOaVAAIMZoQZkX2dmdWyfjk068uDnc7PV6LkoAkEMIDAEUCZlGGImZmRUCAEQoWrEKxoBvSmfiduKKpgoQaoE6SKSMBi2gPIcApJQanR7t1+M3d+4PBssbG1vtdieOYlHaiyIiE4QBgDhU2dwYQwhGNYJgLRowwAwobWeDAlbgmQM1CpUEJiDSAgQigGiD59n+ZOfgXuTiNE3jOI6cs9aa2jeIyCwiEuqyrAvrrEYVGSvIKJqZhYkFCgoNsFK6QbSCotgrEgGlNWClFAgAs2rKbJYTUAN1hoimqCqjFLAB5jI/dE76q5sJgaKgEyfKn03GZTY7v31l7luTyVkUpd43CMQiEIBFSMCBVzoEj8QKUEmd83R3vPc2iDIUAgj0omShlZapAWxsVsZBraysVEncBJ/EqU6TdGGh21pfW66ZuRIpmEfHhz6fWvEmVJob7+dGpwwxKwPlfLZ/r54cZlltIDSLaaebmr2D+6WLago42tleWlnZGt7e3xfGNC8XW/Gru6+01/J2ZO++cZNave7lJ9obj+Q7t3Q2W5CsyKbF/MjZ9qzSSXewlGAGHhBQKaPIr7Xbh5Mj30HT6SjUwU/OP/XYBLjppRqNWoins/m8KrmY1lVYXIh3syw/Hp/vdjeuPDG9WeV7O5PDnVk+pqDOSkx6g87WIBSzqqyV0qq/0FludyKRfmzXLa5KeM+lKxfXt8RTN3IJhpW17vDi+mC41O67zJ/0V3pL/QVF+enkOFPJ5qNP1xJXZWE1KhTNTT09Ot59OxSF0goAzPm1/pc+84mdty/Mq6yumlCHCxvnhEWW1858kxfZ5vJKEM7ySuKoLT3NtLqY5EfH2V7ha26tbm489iz7s6P9t4psDkwLLW2gFAO+IAE0C7r60FPnPvDYcF7UXpQPEoqyrOrtZljUlOWltWYym8Xbrqxr6S7vjQ7u3L3/aG/l/vEpsKa40z7/1LOXLpzuvvX6yy8djV5v4QTqvCKNzMZqk51OHty9sTncHq6vmrTDaGYnJ9PpZKm/lJe+KJs8y+fZ4pVLF/M8r8pykES29u/94DOnhb83OmtUTGUFvcHGE9uDJz4VJoent/5x98a/Tt56Q7lcGTbdpDUfjw6Yl9dwUZt2pwuLHY2+k8BiuyPKBd/cunl7MBik6bkiy5+8MPzY+54qgxQBLm/R4bjcH52O7u7eJ6nSTtLd7L7r0+++8qHh3evXX/jD8egufvvrX7v6/osEdvdwdu3G66vDrWc/9tHhYNFQoU0CyhljAFUStyPnhAh88CTz0peEt+7c29k56PcGWTa7ezC6tbP7ytzMo+7yQvroakuP71178c/4hY8//fi5lcWlwUuv3b59596Hn7saQD5/9SO9WOKkY2xaVsVgaSWNWk1dAwBq5UGhje/sPPjOd793cnT6wac/8rmvfFXq6sa//rkf8LUps46knF4+t7J352VzPC1u22N9NL5/cPDRqx//xre++YMf/uj3v33+ncMl63Srs0BE/cX+oL9qjHHOKTQZhcaoH//k5zdvvxpZ9+vnf7l55fHHL78jieIFCRttCEblhNLU54fnzPDCIwRz7yvXaq9vDQVla2PzL7/51XzUS5MoShIAjIxtp+00SZ11sUskjo7L+Wu3bn7yk1effPeTP/3Zz1/86x8vrnVdqk9Go1fuvGFbyepCl0pKnDIBiFhclLYWYJYVh0fHJ6eTB6OxBB9HifckAJE1rchqo5M4juOUNd4/PgTBL3zxi88888zu7oNfP//ba6+cp6qZHJ414z1DnSJkb09208iZk+nYh8ooJYGuXb/x+JPvvXb9VQ+qMUnj9cHBSVVXzhirAQGss9YaEs6qsr+8ury0NJ/N1tbXTifHf/rTH6osH4+zHJVJIi3YWx2srK4ZQkbtsqIos2x0PP7+D3648+ZO1tCbe8cPo8ETI9UaFAJiSYIBAUAkadXj8ThybnY2q+tw794DDOQZJE4FwFnXitpFTqa/1AfQZZbXrbZCNZ1MlwYri/1BYGFpgq8pBO+JvRBRXTcsAsIK1HQ2+9sLf3vuuedeu3mLCBoWDZpReWKqPTSyu7Orow5+9sufZQYg0GCMMSgAgZhFaR2agqkhYmYWgeBDlmd1XXvfUKC6rtMkubC9/e+XXp7OKgQUERIRBEAEAKV0nKYGUVurUCMQWmtBQBAjrQHRGUCIgw/EDCJK66XlvvdBhImYmfK8GB0eXriwPc99UZYAEkRIWJiV1koppdCIaGFEQERgZmstGI2IChGM1kpZFu89EQGCsGi0gYLWYJVKOt3hOccsZUPeB2ZGrR4GndaaiOq6Nk1FiKgVWKWYWRuDRgsIgyAqhdYmVrSPtHqYnSISQvBNw8IhhKJhIqqCR0TQKETC7JwzxgBAmqbmoTMKBEhRFHnvibx1lpkNWPIhCIgIgyiFiIhK2Uhr6xCRiJjZB684MFEg0oIcwv9DWCn1H6uUEMHk1ZAkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x1CB579592E8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.preprocessing.image.array_to_img(x_train[0]*255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = './gan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0419 10:05:42.724776 171240 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss:  0.6817627\n",
      "adversarial loss:  0.68498296\n",
      "discriminator loss:  0.70677006\n",
      "adversarial loss:  0.7218216\n",
      "discriminator loss:  0.6910426\n",
      "adversarial loss:  0.7746408\n",
      "discriminator loss:  0.7054717\n",
      "adversarial loss:  0.70951337\n",
      "discriminator loss:  0.704112\n",
      "adversarial loss:  0.76954114\n",
      "discriminator loss:  0.7092732\n",
      "adversarial loss:  0.96290225\n",
      "discriminator loss:  0.70618474\n",
      "adversarial loss:  0.7908902\n",
      "discriminator loss:  0.70501465\n",
      "adversarial loss:  0.75790775\n",
      "discriminator loss:  0.69745874\n",
      "adversarial loss:  0.8117298\n",
      "discriminator loss:  0.7047971\n",
      "adversarial loss:  0.8500973\n",
      "discriminator loss:  0.7050698\n",
      "adversarial loss:  0.80390626\n",
      "discriminator loss:  0.703355\n",
      "adversarial loss:  0.73567843\n",
      "discriminator loss:  0.69697577\n",
      "adversarial loss:  0.71096545\n",
      "discriminator loss:  0.70255494\n",
      "adversarial loss:  0.755379\n",
      "discriminator loss:  0.6948449\n",
      "adversarial loss:  0.7801098\n",
      "discriminator loss:  0.7066632\n",
      "adversarial loss:  0.7891054\n",
      "discriminator loss:  0.69195205\n",
      "adversarial loss:  0.8236848\n",
      "discriminator loss:  0.7094645\n",
      "adversarial loss:  0.7626949\n",
      "discriminator loss:  0.70316863\n",
      "adversarial loss:  0.7341591\n",
      "discriminator loss:  0.7060976\n",
      "adversarial loss:  0.72662324\n",
      "discriminator loss:  0.7024553\n",
      "adversarial loss:  0.73630714\n",
      "discriminator loss:  0.70223254\n",
      "adversarial loss:  0.8073357\n",
      "discriminator loss:  0.70038015\n",
      "adversarial loss:  0.69925004\n",
      "discriminator loss:  0.694177\n",
      "adversarial loss:  0.76934737\n",
      "discriminator loss:  0.6976585\n",
      "adversarial loss:  0.735451\n",
      "discriminator loss:  0.7023696\n",
      "adversarial loss:  0.7903887\n",
      "discriminator loss:  0.7041961\n",
      "adversarial loss:  0.93271464\n",
      "discriminator loss:  0.7087578\n",
      "adversarial loss:  0.70520115\n",
      "discriminator loss:  0.6973066\n",
      "adversarial loss:  0.7728162\n",
      "discriminator loss:  0.7051665\n",
      "adversarial loss:  0.6139828\n",
      "discriminator loss:  0.69587815\n",
      "adversarial loss:  0.7199893\n",
      "discriminator loss:  0.7228154\n",
      "adversarial loss:  0.7842046\n",
      "discriminator loss:  0.7077542\n",
      "adversarial loss:  0.768461\n",
      "discriminator loss:  0.7095006\n",
      "adversarial loss:  0.88620794\n",
      "discriminator loss:  0.71087945\n",
      "adversarial loss:  0.7164355\n",
      "discriminator loss:  0.72119\n",
      "adversarial loss:  0.8193871\n",
      "discriminator loss:  0.7050654\n",
      "adversarial loss:  0.8057001\n",
      "discriminator loss:  0.6962527\n",
      "adversarial loss:  0.685311\n",
      "discriminator loss:  0.70855796\n",
      "adversarial loss:  0.7701411\n",
      "discriminator loss:  0.6913292\n",
      "adversarial loss:  0.7914336\n",
      "discriminator loss:  0.69972086\n",
      "adversarial loss:  0.9178623\n",
      "discriminator loss:  0.6792067\n",
      "adversarial loss:  0.77730596\n",
      "discriminator loss:  0.7131923\n",
      "adversarial loss:  0.75595695\n",
      "discriminator loss:  0.71284664\n",
      "adversarial loss:  0.8587289\n",
      "discriminator loss:  0.71838915\n",
      "adversarial loss:  0.71581954\n",
      "discriminator loss:  0.6963562\n",
      "adversarial loss:  0.7420064\n",
      "discriminator loss:  0.709014\n",
      "adversarial loss:  0.69723874\n",
      "discriminator loss:  0.7093079\n",
      "adversarial loss:  0.7032925\n",
      "discriminator loss:  0.7032572\n",
      "adversarial loss:  0.8088356\n",
      "discriminator loss:  0.7086679\n",
      "adversarial loss:  0.81219685\n",
      "discriminator loss:  0.6948002\n",
      "adversarial loss:  0.7338545\n",
      "discriminator loss:  0.70116794\n",
      "adversarial loss:  0.62029636\n",
      "discriminator loss:  0.70727956\n",
      "adversarial loss:  0.84658176\n",
      "discriminator loss:  0.71784544\n",
      "adversarial loss:  0.8328748\n",
      "discriminator loss:  0.69322157\n",
      "adversarial loss:  0.8057047\n",
      "discriminator loss:  0.697255\n",
      "adversarial loss:  0.80669993\n",
      "discriminator loss:  0.69929063\n",
      "adversarial loss:  0.7807911\n",
      "discriminator loss:  0.7232787\n",
      "adversarial loss:  0.8445732\n",
      "discriminator loss:  0.7266045\n",
      "adversarial loss:  0.8714186\n",
      "discriminator loss:  0.69305724\n",
      "adversarial loss:  0.72019386\n",
      "discriminator loss:  0.70892537\n",
      "adversarial loss:  0.74839616\n",
      "discriminator loss:  0.7137997\n",
      "adversarial loss:  0.87032926\n",
      "discriminator loss:  0.73193026\n",
      "adversarial loss:  0.7782806\n",
      "discriminator loss:  0.7217245\n",
      "adversarial loss:  0.6825167\n",
      "discriminator loss:  0.728211\n",
      "adversarial loss:  0.63462955\n",
      "discriminator loss:  0.7139516\n",
      "adversarial loss:  0.898625\n",
      "discriminator loss:  0.69879115\n",
      "adversarial loss:  0.83153534\n",
      "discriminator loss:  0.7314816\n",
      "adversarial loss:  0.7801775\n",
      "discriminator loss:  0.71834236\n",
      "adversarial loss:  0.78725356\n",
      "discriminator loss:  0.705697\n",
      "adversarial loss:  0.63391674\n",
      "discriminator loss:  0.71234363\n",
      "adversarial loss:  0.67251194\n",
      "discriminator loss:  0.7007787\n",
      "adversarial loss:  0.8382627\n",
      "discriminator loss:  0.70886385\n",
      "adversarial loss:  0.6987278\n",
      "discriminator loss:  0.70229703\n",
      "adversarial loss:  0.89380944\n",
      "discriminator loss:  0.6960679\n",
      "adversarial loss:  0.7113008\n",
      "discriminator loss:  0.70328844\n",
      "adversarial loss:  0.7289411\n",
      "discriminator loss:  0.7121952\n",
      "adversarial loss:  0.79581785\n",
      "discriminator loss:  0.72927237\n",
      "adversarial loss:  0.65820086\n",
      "discriminator loss:  0.72086096\n",
      "adversarial loss:  0.8578247\n",
      "discriminator loss:  0.72797626\n",
      "adversarial loss:  0.6389435\n",
      "discriminator loss:  0.7130507\n",
      "adversarial loss:  0.644551\n",
      "discriminator loss:  0.71077096\n",
      "adversarial loss:  0.78249824\n",
      "discriminator loss:  0.71992266\n",
      "adversarial loss:  0.7450329\n",
      "discriminator loss:  0.7093162\n",
      "adversarial loss:  0.8816603\n",
      "discriminator loss:  0.7147338\n",
      "adversarial loss:  0.8867341\n",
      "discriminator loss:  0.73730934\n",
      "adversarial loss:  0.8289118\n",
      "discriminator loss:  0.73706025\n",
      "adversarial loss:  0.81512815\n",
      "discriminator loss:  0.7015322\n",
      "adversarial loss:  0.7604753\n",
      "discriminator loss:  0.7000402\n",
      "adversarial loss:  0.98633015\n",
      "discriminator loss:  0.69475526\n",
      "adversarial loss:  0.76383257\n",
      "discriminator loss:  0.6982854\n",
      "adversarial loss:  0.90862405\n",
      "discriminator loss:  0.7113298\n",
      "adversarial loss:  0.8216995\n",
      "discriminator loss:  0.70247686\n",
      "adversarial loss:  0.7110063\n",
      "discriminator loss:  0.73193634\n",
      "adversarial loss:  0.8767713\n",
      "discriminator loss:  0.7096117\n",
      "adversarial loss:  0.75650847\n",
      "discriminator loss:  0.70139515\n",
      "adversarial loss:  0.82936764\n",
      "discriminator loss:  0.7169457\n",
      "adversarial loss:  0.79926646\n",
      "discriminator loss:  0.72275007\n",
      "adversarial loss:  0.7700857\n",
      "discriminator loss:  0.69440544\n",
      "adversarial loss:  0.9196981\n",
      "discriminator loss:  0.71815526\n",
      "adversarial loss:  0.7670941\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start:stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    labels = np.concatenate([\n",
    "        np.ones((batch_size, 1)), \n",
    "        np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    \n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('models/gan.h5')\n",
    "        print('discriminator loss: ', d_loss)\n",
    "        print('adversarial loss: ', a_loss)\n",
    "        \n",
    "        img = keras.preprocessing.image.array_to_img(generated_images[0] * 255.,scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_img' + str(step) + '.png'))\n",
    "        \n",
    "        img = keras.preprocessing.image.array_to_img(real_images[0] * 255.,scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_img' + str(step) + '.png'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 2, 2, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 4, 4, 1)           10        \n",
      "=================================================================\n",
      "Total params: 10\n",
      "Trainable params: 10\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(2,2,1,))\n",
    "#x = layers.Dense(10)(input_tensor)\n",
    "#x = layers.Conv2D(1, 2)(input_tensor)\n",
    "x = layers.Conv2DTranspose(1,3)(input_tensor)\n",
    "model = models.Model(input_tensor, x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5570464730262756"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0,0,0,1,2,0,3,4,0]).dot(model.get_weights()[0].reshape(9)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.324037  , -0.19854662, -0.44006208,  0.32107568, -0.41231918,\n",
       "        0.4104722 , -0.49147338,  0.3552341 , -0.13237336], dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()[0].reshape(9)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = layers.Conv2D(3,1,input_shape=(3,3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.13237336],\n",
       "         [ 0.09048736],\n",
       "         [ 0.2189948 ],\n",
       "         [-0.98294675]],\n",
       "\n",
       "        [[ 0.01335213],\n",
       "         [ 0.944834  ],\n",
       "         [-0.5570464 ],\n",
       "         [-1.3237422 ]],\n",
       "\n",
       "        [[ 0.79135466],\n",
       "         [-0.67373943],\n",
       "         [-1.40718   ],\n",
       "         [ 0.63622874]],\n",
       "\n",
       "        [[-1.3201863 ],\n",
       "         [-2.3558881 ],\n",
       "         [-1.7662975 ],\n",
       "         [-1.296148  ]]]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.arange(1,5).reshape(-1,2,2,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
